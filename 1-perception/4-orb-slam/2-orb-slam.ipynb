{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explanation\n",
        "\n",
        "Early SLAM approaches depended on LiDAR and were relatively naive; for example, they didn't take advantage of loop closures (when the robot returns to a position it has previously been at) to resolve errors in the robots trajectory and environment map.\n",
        "\n",
        "ORB-SLAM instead provides an advanced SLAM approach that depends on only a single camera, which is far cheaper, effectively uses loop closures, and offers more robustness against failure.\n",
        "\n",
        "They accomplish this by detecting ORB image features in a series of frames, using a keyframe filtering system to only save important frames from a video feed that contain sufficient novel information, and implement other algorithmic improvements.\n",
        "\n",
        "With this design, ORB-SLAM based approaches (most recently ORB-SLAM-3) are at the SOTA performance for SLAM solutions. Notably, deep learning based approaches with low priors still haven't fully surpassed the computational SLAM approaches like ORB-SLAM with far more inductive bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notes\n",
        "\n",
        "> Visual SLAM has the goal of estimating the camera trajectory while reconstructing the environment.\n",
        "\n",
        "Visual SLAM is challenging as it requires efficient usage of a subset of observations and keyframes to prevent redundancy as complexity grows, a strong network of observations to produce accurate results, sufficient loop-closure abilities, handling occlusions, etc.\n",
        "\n",
        "ORB-SLAM is a new monocular SLAM algorithm that:\n",
        "\n",
        "- Uses a single set of ORB features for all tasks: tracking, mapping, re-localization, and loop closing.\n",
        "- Operates in real-time in large environments using a co-visibility graph.\n",
        "- Real-time loop closing based on pose graph optimization.\n",
        "- Real-time camera relocalization which allows recovery from tracking failure.\n",
        "- A new initialization procedure.\n",
        "- A survival of the fittest map point and keyframe selection approach.\n",
        "\n",
        "> To the best of our knowledge, this is the most complete and reliable solution to monocular SLAM, and for the benefit of the community we make the source code public.\n",
        "\n",
        "### Related Work\n",
        "\n",
        "**1. Place Recognition**\n",
        "\n",
        "> [Place recognition approaches] based on appearance, that is image to image matching, scale better in large environments than map to map or image to map methods.\n",
        "\n",
        "> With appearance based methods, bag of words techniques, are to the fore because of their high efficiency.\n",
        "\n",
        "**2. Map Initialization**\n",
        "\n",
        "> Monocular SLAM requires a procedure to create an initial map because depth cannot be recovered from a single image.\n",
        "\n",
        "**3. Monocular SLAM**\n",
        "\n",
        "> Monocular SLAM was initially solved by filtering, [where] every frame is processed by the filter to jointly estimate the map feature locations and the camera pose.\n",
        "\n",
        "> It has the drawbacks of wasting computation in processing consecutive frames with little new information and the accumulation of linearization errors.\n",
        "\n",
        "> On the other hand keyframe-based approaches, estimate the map using only selected frames (keyframes) allowing to perform more costly but accurate bundle adjustment optimizations.\n",
        "\n",
        "> Keyframe-based techniques are more accurate than filtering for the same computational cost.\n",
        "\n",
        "> [PTAM] was the first work to introduce the idea of splitting camera tracking and mapping in parallel threads, and demonstrated to be successful for real time augmented reality applications in small environments.\n",
        "\n",
        "> In our system we take advantage of the excellent ideas of using a local map based on co-visibility, and building the pose graph from the\n",
        "> co-visibility graph, but apply them in a totally redesigned frontend and back-end.\n",
        "\n",
        "> Another difference is that, instead of using specific features for loop detection (SURF), we perform the place recognition on the same tracked and mapped features, obtaining robust frame-rate relocalization and loop detection.\n",
        "\n",
        "> All visual SLAM works in the literature agree that running BA with all the points and all frames is not feasible.\n",
        "\n",
        "> The most cost effective approach is to keep as much points as possible, while keeping only non-redundant keyframes.\n",
        "\n",
        "> Our survival of the fittest strategy achieves unprecedented robustness in difficult scenarios by inserting keyframes as quickly as possible, and removing later the redundant ones, to avoid the extra cost.\n",
        "\n",
        "### System Overview\n",
        "\n",
        "**1. Feature Choice**\n",
        "\n",
        "> One of the main design ideas in our system is that the same features used by the mapping and tracking are used for place recognition to perform frame-rate relocalization and loop detection.\n",
        "\n",
        "They use the same features for all tasks which makes their algorithm far more computationally efficient.\n",
        "\n",
        "> [ORB features] are extremely fast to compute and match, while they have good invariance to viewpoint.\n",
        "\n",
        "They use ORB to extract features which has high performance.\n",
        "\n",
        "**2. Three Threads: Tracking, Local Mapping, and Loop Closing**\n",
        "\n",
        "![Screenshot 2024-11-07 at 2.14.14 PM.png](../../images/Screenshot_2024-11-07_at_2.14.14_PM.png)\n",
        "\n",
        "> Our system, see an overview in Fig. 1, incorporates three threads that run in parallel: tracking, local mapping and loop closing.\n",
        "\n",
        "> The tracking is in charge of localizing the camera with every frame and deciding when to insert a new keyframe.\n",
        "\n",
        "> The local mapping processes new keyframes and performs local BA to achieve an optimal reconstruction in the surroundings of the camera pose.\n",
        "\n",
        "> The local mapping is also in charge of culling redundant keyframes.\n",
        "\n",
        "> The loop closing searches for loops with every new keyframe. If a loop is detected, we compute a similarity transformation that informs about the drift accumulated in the loop. Then both sides of the loop are aligned and duplicated points are fused.\n",
        "\n",
        "**3. Map Points, Key Frames, and their Selection**\n",
        "\n",
        "Each map point $p_i$ stores:\n",
        "\n",
        "- Its 3D position within world coordinates\n",
        "- It’s viewing direction\n",
        "- A representative ORB descriptor for the point\n",
        "- The max and min distances from which it can be observed\n",
        "\n",
        "Each keyframe $K_i$ stores:\n",
        "\n",
        "- The camera pose that transforms points from the world to camera coordinates\n",
        "- The camera intrinsics like focal length and principle point\n",
        "- All the ORB features extracted in the frame.\n",
        "\n",
        "> Map points and keyframes are created with a generous policy, while a later very exigent culling mechanism is in charge of detecting redundant keyframes and wrongly matched or not trackable map points.\n",
        "\n",
        "> This permits a flexible map expansion during exploration, which promotes robustness under hard conditions.\n",
        "\n",
        "**4. Covisibility Graph and Essential Graph**\n",
        "\n",
        "> Covisibility information between keyframes is very useful in several tasks of our system, and is represented as an undirected weighted graph.\n",
        "\n",
        "> Each node is a keyframe and an edge between two keyframes exists if they share observations of the same map points.\n",
        "\n",
        "They use this covisibility graph for loop closure.\n",
        "\n",
        "**5. Bag of Words Place Recognition**\n",
        "\n",
        "> The system has embedded a bags of words place recognition module, to perform loop detection and localization.\n",
        "\n",
        "> Visual words are just a discretization of the descriptor space, which is known as the visual vocabulary. The vocabulary is created offline with the ORB descriptors extracted from a large set of images.\n",
        "\n",
        "> If the images are general enough, the same vocabulary can be used for different environments getting a good performance.\n",
        "\n",
        "### Automatic Map Initialization\n",
        "\n",
        "> The goal of the map initialization is to compute the relative pose between two frames to triangulate an initial set of map points.\n",
        "\n",
        "![Screenshot 2024-11-07 at 2.26.35 PM.png](../../images/Screenshot_2024-11-07_at_2.26.35_PM.png)\n",
        "\n",
        "### Tracking\n",
        "\n",
        "The tracking thread steps are performed at every camera frame.\n",
        "\n",
        "**1. ORB Extraction**\n",
        "\n",
        "They first extract FAST corners from each section of the grid. Then they compute ORB descriptors and orientations using the FAST corners.\n",
        "\n",
        "**2. Initial Pose Estimation from Previous Frame**\n",
        "\n",
        "If tracking in the last frame was successful, they use constant velocity to predict the camera pose and search for observed map points that should be visible.\n",
        "\n",
        "**3. Initial Pose Estimation via Global Relocalization**\n",
        "\n",
        "If they lose tracking, they convert the frame into a bag of words and query the recognition database for keyframes that they can use for relocalization.\n",
        "\n",
        "**4. Track Local Map**\n",
        "\n",
        "> Once we have an estimation of the camera pose and an initial set of feature matches, we can project the map into the frame and search more map point correspondences.\n",
        "\n",
        "They only use a local map to prevent complexity in large environments.\n",
        "\n",
        "> The camera pose is finally optimized with all the map points found in the frame\n",
        "\n",
        "**5. New Keyframe Decision**\n",
        "\n",
        "Then the system has to decide whether to insert the current frame as a keyframe.\n",
        "\n",
        "> As there is a mechanism in the local mapping to cull redundant keyframes, we will try to insert keyframes as fast as possible, because that makes the tracking more robust to challenging camera movements, typically rotations.\n",
        "\n",
        "They want to be generous with storing keyframes because it helps with fast camera movements that are otherwise hard to recover.\n",
        "\n",
        "### Local Mapping\n",
        "\n",
        "These are steps performed with every new keyframe.\n",
        "\n",
        "**1. Keyframe Insertion**\n",
        "\n",
        "They add the keyframe to the covisibility graph and store the bag of words representation of the keyframe.\n",
        "\n",
        "**2. Recent Map Points Culling**\n",
        "\n",
        "> Map points, in order to be retained in the map, must pass a restrictive test during the first three keyframes after creation.\n",
        "\n",
        "**3. New Map Point Creation**\n",
        "\n",
        "> New map points are created by triangulating ORB from connected keyframes in the covisibility graph.\n",
        "\n",
        "**4. Local Bundle Adjustment**\n",
        "\n",
        "> The local BA optimizes the currently processes keyframe, all the keyframes connected to it in the covisibility graph, and all the map points seen by those keyframes.\n",
        "\n",
        "**5. Local Keyframe Culling**\n",
        "\n",
        "> In order to maintain a compact reconstruction, the local mapping tries to detect redundant keyframes and delete them.\n",
        "\n",
        "> We discard all the keyframes in Kc whose 90% of the map points have been seen in at least other three keyframes in the same or finer scale.\n",
        "\n",
        "### Loop Closing\n",
        "\n",
        "**1. Loop Candidates Detection**\n",
        "\n",
        "> At first we compute the similarity between the bag of words vector of $K_i$ and all its neighbors in the covisibility graph.\n",
        "\n",
        "They query the recognition database to find similar keyframes and delete frames whose score is lower than some threshold.\n",
        "\n",
        "> To accept a loop candidate we must detect consecutively three\n",
        "> loop candidates that are consistent.\n",
        "\n",
        "**2. Compute the Similarity Transformation**\n",
        "\n",
        "They compute a similarity transformation between the frames.\n",
        "\n",
        "**3. Loop Fusion**\n",
        "\n",
        "> The first step in the loop correction is to fuse duplicated map points and insert new edges in the covisibility graph.\n",
        "\n",
        "**4. Essential Graph Optimization**\n",
        "\n",
        "> To effectively close the loop, we perform a pose graph optimization over the Essential Graph that distributes the loop closing error along the graph.\n",
        "\n",
        "### Experiments\n",
        "\n",
        "> Our system runs in real time and processes the images exactly at the frame rate they were acquired.\n",
        "\n",
        "> ORB-SLAM has three main threads, that run in parallel with other tasks from ROS and the operating system.\n",
        "\n",
        "**1. System Performance in the NewCollege Dataset**\n",
        "\n",
        "![Screenshot 2024-11-07 at 2.51.05 PM.png](../../images/Screenshot_2024-11-07_at_2.51.05_PM.png)\n",
        "\n",
        "> The NewCollege dataset contains a 2.2km sequence from a robot traversing a campus and adjacent parks.\n",
        "\n",
        "Since ORB-SLAM depends purely on visual data, they can construct the environment just from a video. Very cool.\n",
        "\n",
        "> It contains several loops and fast rotations that makes the sequence quite challenging for monocular vision. To the best of our knowledge there is no other monocular system in the literature able to process this whole sequence.\n",
        "\n",
        "Loop closures and fast turning make video data difficult to process.\n",
        "\n",
        "**2. Localization Accuracy in the TUM RGB-D Benchmark**\n",
        "\n",
        "> The TUM RGB-D benchmark is an excellent dataset to evaluate the accuracy of camera localization as it provides several sequences with accurate ground truth obtained with an external motion capture system.\n",
        "\n",
        "This dataset is good for evaluating performance because it comes with location data.\n",
        "\n",
        "> In terms of accuracy ORB-SLAM and PTAM are similar in open trajectories, while ORB-SLAM achieves higher accuracy when detecting large loops.\n",
        "\n",
        "**3. Relocalization in the TUM RGB-D Benchmark**\n",
        "\n",
        "> ORB-SLAM accurately relocalizes more than the double of frames than PTAM.\n",
        "\n",
        "**4. Lifelong Experiment in the TUM RGB-D Benchmark**\n",
        "\n",
        "> Previous relocalization experiments have shown that our system is able to localize in a map from very different viewpoints and robustly under moderate dynamic changes.\n",
        "\n",
        "> This property in conjunction with our keyframe culling procedure allows to operate lifelong in the same environment under different viewpoints and some dynamic changes.\n",
        "\n",
        "> While the lifelong operation in a static scenario should be a requirement of any SLAM system, more interesting is the case where dynamic changes occur.\n",
        "\n",
        "**5. Large Scale and Large Loop Closing in the KITTI Dataset**\n",
        "\n",
        "> 11 sequences from a car driven around a residential area with accurate ground truth from GPS and a Velodyne laser scanner.\n",
        "\n",
        "> This is a very challenging dataset for monocular vision due to fast rotations, areas with lot of foliage, which make more difficult data association, and relatively high car speed.\n",
        "\n",
        "![Screenshot 2024-11-07 at 2.56.51 PM.png](../../images/Screenshot_2024-11-07_at_2.56.51_PM.png)\n",
        "\n",
        "### Conclusions and Discussion\n",
        "\n",
        "**1. Conclusion**\n",
        "\n",
        "> In this work we have presented a new monocular SLAM system with a detailed description of its building blocks and an exhaustive evaluation in public datasets.\n",
        "\n",
        "> The accuracy of the system is typically below 1 cm.\n",
        "\n",
        "> The main contribution of our work is to expand the versatility of PTAM to environments that are intractable for that system.\n",
        "\n",
        "> To the best of our knowledge, no other system has demonstrated to work in as many different scenarios and with such accuracy. Therefore our system is currently the most reliable and complete solution for monocular SLAM.\n",
        "\n",
        "> Finally we have also demonstrated that ORB features have enough recognition power to enable place recognition from severe viewpoint change.\n",
        "\n",
        "**2. Sparse/Feature-based vs. Dense/Direct Methods**\n",
        "\n",
        "This is a sparse/feature-based SLAM method. There are also dense methods that perform dense reconstruction of the environment and localizing the camera by optimizing over image pixel intensities.\n",
        "\n",
        "> In contrast, feature-based methods are able to match features with a wide baseline, thanks to their good invariance to viewpoint and illumination changes.\n",
        "\n",
        "> We consider that the future of monocular SLAM should incorporate the best of both approaches.\n",
        "\n",
        "**3. Future Work**\n",
        "\n",
        "> The accuracy of our system can still be improved incorporating points at infinity in the tracking.\n",
        "\n",
        "> Another open way is to upgrade the sparse map of our system to a denser and more useful reconstruction.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
