{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "\n",
    "High sample complexity and bad convergence properties (non-stationarity, high correlation between time steps) make it challenging to train deep reinforcement learning policies for control tasks.\n",
    "\n",
    "> In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework.\n",
    "\n",
    "SAC is a max entropy form of DDPG that works in continuous action spaces and has exploration built into its objective function.\n",
    "\n",
    "> Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.\n",
    "\n",
    "> One cause for the poor sample efficiency of deep RL methods is on-policy learning: some of the most commonly used deep RL algorithms, such as TRPO, PPO, A3C require new samples to be collected for each gradient step. This quickly becomes extravagantly expensive, as the number of gradient steps and samples per step needed to learn\n",
    "> an effective policy increases with task complexity.\n",
    "\n",
    "On-policy methods are highly sample inefficient.\n",
    "\n",
    "> Unfortunately, the combination of off-policy learning and high-dimensional, nonlinear function approximation with neural networks presents a major challenge for stability and convergence.\n",
    "\n",
    "Off-policy learning is really hard though especially with the poor convergence properties of deep reinforcement learning. Shifting the policy and training on old policy data is really hard.\n",
    "\n",
    "> In this paper, we demonstrate that we can devise an off-policy maximum entropy actor-critic algorithm, which we call soft actor-critic (SAC), which provides for both sample efficient learning and stability.\n",
    "\n",
    "The main properties of SAC: off-policy, sample efficient, actor-critic, max entropy, with good convergence properties.\n",
    "\n",
    "### Soft Actor-Critic\n",
    "\n",
    "Our off-policy soft actor-critic algorithm can be derived starting from a maximum entropy variant of the policy iteration method.\n",
    "\n",
    "> Large continuous domains require us to derive a practical approximation to soft policy iteration.\n",
    "\n",
    "![Screenshot 2024-12-05 at 7.01.02 PM.png](../../../images/Screenshot_2024-12-05_at_7.01.02_PM.png)\n",
    "\n",
    "> Our algorithm also makes use of two Q-functions to mitigate positive bias in the policy improvement step that is known to degrade performance of value based method.\n",
    "\n",
    "They use two Q-functions to prevent overestimation bias and improve convergence properties without requiring completely separate target networks.\n",
    "\n",
    "> Although our algorithm can learn challenging tasks, including a 21-dimensional Humanoid, using just a single Q-function, we found two Q-functions significantly speed up training, especially on harder task.\n",
    "\n",
    "### Experiments\n",
    "\n",
    "> The results show that, overall, SAC performs comparably to the baseline methods on the easier tasks and outperforms them on the harder tasks with a large margin, both in terms of learning speed and the final performance.\n",
    "\n",
    "![Screenshot 2024-12-05 at 7.02.12 PM.png](../../../images/Screenshot_2024-12-05_at_7.02.12_PM.png)\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "> In fact, the sample efficiency of this approach actually exceeds that of DDPG by a substantial margin. Our results suggest that stochastic, entropy maximizing reinforcement learning algorithms can provide a promising avenue for improved robustness and stability.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
