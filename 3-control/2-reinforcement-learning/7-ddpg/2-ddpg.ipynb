{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DDPG\n",
        "\n",
        "> We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces.\n",
        "\n",
        "> However, while DQN solves problems with high-dimensional observation spaces, it can only handle discrete and low-dimensional action spaces.\n",
        "\n",
        "Prior deep reinforcement learning approaches required limited action spaces. DDPG is meant to work for continuous domains.\n",
        "\n",
        "It’s too hard to discretize the action space when there are many degrees of freedom. The action space quickly explodes. Instead they create an off-policy model-free actor-critic algorithm that can learn policies in high-dimensional continuous action spaces.\n",
        "\n",
        "> A key feature of the approach is its simplicity: it requires only a straightforward actor-critic architecture and learning algorithm with very few “moving parts”, making it easy to implement and scale to more difficult problems and larger networks.\n",
        "\n",
        "They combine DPG with DQN.\n",
        "\n",
        "### Algorithm\n",
        "\n",
        "> The DPG algorithm maintains a parameterized actor function $\\mu(s | \\theta^\\mu)$ which specifies the current policy by deterministically mapping states to a specific action.\n",
        "\n",
        "Like in DQN, they use a replay buffer to address the issues of non-stationarity and correlation between time steps. DDPG is off-policy so the replay buffer can be large.\n",
        "\n",
        "They create clones of the actor-critic networks to smooth out the actor and critic network updates and prevent them from diverging.\n",
        "\n",
        "They also use batch normalization across mini-batches to make the batch samples have unit mean and variance.\n",
        "\n",
        "### Results\n",
        "\n",
        "![Screenshot 2024-12-05 at 6.40.01 PM.png](../../../images/notes/Screenshot_2024-12-05_at_6.40.01_PM.png)\n",
        "\n",
        "> In particular, learning without a target network, as in the original work with DPG, is very poor in many environments.\n",
        "\n",
        "The target network is an essential update to make training reliable.\n",
        "\n",
        "![Screenshot 2024-12-05 at 6.41.24 PM.png](../../../images/notes/Screenshot_2024-12-05_at_6.41.24_PM.png)\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "> The work combines insights from recent advances in deep learning and reinforcement learning, resulting in an algorithm that robustly solves challenging problems across a variety of domains with continuous action spaces, even when using raw pixels for observations.\n",
        "\n",
        "> Interestingly, all of our experiments used substantially fewer steps of experience than was used by DQN learning to find solutions in the Atari domain\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
