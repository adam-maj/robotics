{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "> Its promise is beguiling|a way of programming agents by reward and punishment without needing to specify how the task is to be achieved.\n",
    "> \n",
    "\n",
    "Reinforcement learning is about training algorithms with environments alone, encoding them with minimal priors and using no supervision.\n",
    "\n",
    "> Reinforcement learning is the problem faced by an agent that must learn behavior through trial-and-error interactions with a dynamic environment.\n",
    "> \n",
    "\n",
    "**1. Environment**\n",
    "\n",
    "The world model consists of environment states, actions, and reward signals. The agent needs to find an optimal policy mapping states to actions.\n",
    "\n",
    "The most important difference between RL and supervised learning is that there’s no provision of input/output pairs.\n",
    "\n",
    "We often use a discount factor to reduce the value of future rewards.\n",
    "\n",
    "Many RL algorithms come with a guarantee of converging to optimal over time and a speed of converging. There is also a measure of regret, which is the expected decrease in reward from now executing the optimal policy from the beginning.\n",
    "\n",
    "> Adaptive control is a much more mature discipline that concerns itself with dynamic systems in which states and actions are vectors and system dynamics are smooth: linear or locally linearizable around a desired trajectory.\n",
    "> \n",
    "\n",
    "In robotics, we care about the adaptive control case where reinforcement policies are being used to operate motor control with continuous output values.\n",
    "\n",
    "**2. Exploration**\n",
    "\n",
    "> One major difference between reinforcement learning and supervised learning is that a reinforcement-learner must explicitly explore its environment.\n",
    "> \n",
    "\n",
    "Reinforcement learning algorithms have to explore before they can exploit. Every RL algorithm has to have a way to effectively trade off exploration and exploitation at different points in training.\n",
    "\n",
    "Strategies vary between being purely greedy and using pure randomness.\n",
    "\n",
    "**3. Delayed Reward**\n",
    "\n",
    "The agent has to learn from delayed reinforcement since it may take a long time for it’s actions to actually result in any reward signal. Problems with delayed reinforcement are modeled as a Markov decision process.\n",
    "\n",
    "Given an MDP, we can find the optimal value function using the Bellman equation.\n",
    "\n",
    "$$\n",
    "V_k^*(s) = max_a \\sum_{s’} P(s’|s, a)(R(s,a, s’) + \\gamma V_{k-1}^*(s’))\n",
    "$$\n",
    "\n",
    "With this process, we can iterate through every possible state/action until the values for each state converge.\n",
    "\n",
    "Then a policy can be built to optimize actions to put the agent in states that maximize the value function.\n",
    "\n",
    "**4. Model-free Methods**\n",
    "\n",
    "> The biggest problem facing a reinforcement-learning agent is temporal credit assignment. How do we know whether the action just taken is a good one, when it might have far reaching effects?\n",
    "> \n",
    "\n",
    "To solve this problem, they wait until the end reward is received and then send the reward signal back through previous time-steps.\n",
    "\n",
    "Q-learning optimizes a policy to calculate the value of each action based on a state, so it can be prescriptive.\n",
    "\n",
    "$$\n",
    "Q^*(s, a) = \\sum_{s'} P(s'|s, a)(R(s, a, s') + \\gamma \\textrm{max}_{a'} Q*(s', a'))\n",
    "$$\n",
    "\n",
    "Average reward can be used for a more slowly updating and continuous policy.\n",
    "\n",
    "**6. Generalization**\n",
    "\n",
    "> All of the previous discussion has tacitly assumed that it is possible to enumerate the state and action spaces and store tables of values over them.\n",
    "> \n",
    "\n",
    "Basic RL strategies assume perfect access to the environment dynamics and the ability to explore the entire action space, which is intractable in reality."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
