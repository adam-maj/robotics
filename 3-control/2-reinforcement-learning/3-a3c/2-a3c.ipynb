{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "\n",
    "> We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers.\n",
    "\n",
    "This paper specifically explores using deep reinforcement learning to operate robotic controllers.\n",
    "\n",
    "> The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU.\n",
    "\n",
    "Their actor-critic deep reinforcement learning approach surpasses the prior state of the art, especially for robotic control.\n",
    "\n",
    "> It was previously thought that the combination of simple online RL algorithms with deep neural networks was fundamentally unstable.\n",
    "\n",
    "> Aggregating over memory in this way reduces non-stationarity and de-correlates updates, but at the same time limits the methods to off-policy reinforcement learning algorithms.\n",
    "\n",
    "Using replay memory and sampling randomly across remembered time steps helps to address the issues of high correlation (one time step affected by the prior time step) and non-stationarity (q-values changing as the policy changes) of policies.\n",
    "\n",
    "> Instead of experience replay, we asynchronously execute multiple agents in parallel, on multiple instances of the environment.\n",
    "\n",
    "> Our experiments run on a single machine with a standard multi-core CPU.\n",
    "\n",
    "> Asynchronous advantage actor-critic (A3C), also mastered a variety of continuous motor control tasks as well as learned general strategies for exploring 3D mazes purely from visual inputs.\n",
    "\n",
    "### Asynchronous RL Framework\n",
    "\n",
    "> The aim in designing these methods was to find RL algorithms that can train deep neural network policies reliably and without large resource requirements.\n",
    "\n",
    "> In addition to stabilizing learning, using multiple parallel actor-learners has multiple practical benefits.\n",
    "\n",
    "Training time decreases based on the number of learners used. They can also use on-policy learning methods since they don’t rely on experience replay.\n",
    "\n",
    "They try out 4 different algorithms, including asynchronous advantage actor critic (A3C) maintains a policy and an estimated value function, and then cuts off the reward signal flow with the estimated value function after a certain number of time steps.\n",
    "\n",
    "### Experiments\n",
    "\n",
    "They use their models on Atari and also train robotics control tasks in MuJoCo.\n",
    "\n",
    "> The results show that all four asynchronous methods we presented can successfully train neural network controllers on the Atari domain.\n",
    "\n",
    "![Screenshot 2024-12-05 at 3.35.51 PM.png](../../../images/Screenshot_2024-12-05_at_3.35.51_PM.png)\n",
    "\n",
    "![Screenshot 2024-12-05 at 3.37.01 PM.png](../../../images/Screenshot_2024-12-05_at_3.37.01_PM.png)\n",
    "\n",
    "> We evaluated only the asynchronous advantage actor-critic algorithm since, unlike the value-based methods, it is easily extended to continuous actions. In all problems, using either the physical state or pixels as input, Asynchronous Advantage-Critic found good solutions\n",
    "> in less than 24 hours of training and typically in under a few hours\n",
    "\n",
    "A3C was actually effective in continuous control tasks.\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "> We have presented asynchronous versions of four standard reinforcement learning algorithms and showed that they are able to train neural network controllers on a variety of domains in a stable manner.\n",
    "\n",
    "> One of our main findings is that using parallel actor-learners to update a shared model had a stabilizing effect on the learning process of the three value-based methods we considered\n",
    "\n",
    "> All of the value-based methods we investigated could benefit from different ways of reducing overestimation bias of Q-values.\n",
    "\n",
    "This is exactly the challenge that’s fixed by DDPG.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
