{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notes\n",
        "\n",
        "> We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent.\n",
        "\n",
        "PPO is TRPO with a surrogate objective that can be optimized more easily\n",
        "\n",
        "> We show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.\n",
        "\n",
        "> This paper seeks to improve the current state of affairs by introducing an algorithm that attains the data efficiency and reliable performance of TRPO, while using only first-order optimization.\n",
        "\n",
        "The goal of this paper is to obtain the reliable results that TRPO provides (due to the trust region fixing step-sizing) without the second-order optimization that makes it inefficient.\n",
        "\n",
        "> We propose a novel objective with clipped probability ratios, which forms a pessimistic estimate of the performance of the policy.\n",
        "\n",
        "### Algorithm\n",
        "\n",
        "They use a clipped surrogate objective where they use just the top of TRPO objective and include the TRPO constraint inside the objective itself.\n",
        "\n",
        "$$\n",
        "L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t[\\min(r_t(\\theta)\\hat{A}_t), \\textrm{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t]\n",
        "$$\n",
        "\n",
        "This prevents any of the individual terms from affecting the gradients too much through clipping.\n",
        "\n",
        "They also add the KL term to the objective function.\n",
        "\n",
        "![Screenshot 2024-12-05 at 6.26.13 PM.png](../../../images/notes/Screenshot_2024-12-05_at_6.26.13_PM.png)\n",
        "\n",
        "### Experiments\n",
        "\n",
        "They test on 7 simulated robotics tasks in MuJoCo with OpenAI Gym.\n",
        "\n",
        "![Screenshot 2024-12-05 at 6.30.23 PM.png](../../../images/notes/Screenshot_2024-12-05_at_6.30.23_PM.png)\n",
        "\n",
        "> We see that PPO outperforms the previous methods on almost all the continuous control environments.\n",
        "\n",
        "![Screenshot 2024-12-05 at 6.30.34 PM.png](../../../images/notes/Screenshot_2024-12-05_at_6.30.34_PM.png)\n",
        "\n",
        "> To showcase the performance of PPO on high-dimensional continuous control problems, we train on a set of problems involving a 3D humanoid, where the robot must run, steer, and get up off the ground, possibly while being pelted by cubes.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "> We have introduced proximal policy optimization, a family of policy optimization methods that use multiple epochs of stochastic gradient ascent to perform each policy update.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
