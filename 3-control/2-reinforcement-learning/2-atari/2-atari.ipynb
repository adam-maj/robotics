{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "> We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning.\n",
    "> \n",
    "\n",
    "The first major impressive reinforcement learning model, enabled by recent progress in deep learning with more compute.\n",
    "\n",
    "> The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards.\n",
    "> \n",
    "\n",
    "> We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.\n",
    "> \n",
    "\n",
    "Best reinforcement learning result at the time which surpassed human level skill.\n",
    "\n",
    "> Learning to control agents directly from high-dimensional sensory inputs like vision and speech is one of the long-standing challenges of reinforcement learning.\n",
    "> \n",
    "\n",
    "> Most successful RL applications that operate on these domains have relied on hand-crafted features combined with linear value functions or policy representations.\n",
    "> \n",
    "\n",
    "Manual feature engineering was still being used in early reinforcement learning.\n",
    "\n",
    "At the time of this paper, deep learning had recently shown impressive results. However, deep reinforcement learning posed a number of challenges. It had no access to hand labeled data, had to use reward signals that may be delayed thousands of time steps into the future, and has high correlation between different samples.\n",
    "\n",
    "> This paper demonstrates that a convolutional neural network can overcome these challenges to learn successful control policies from raw video data in complex RL environments.\n",
    "> \n",
    "\n",
    "> Our goal is to create a single neural network agent that is able to successfully learn to play as many of the games as possible\n",
    "> \n",
    "\n",
    "### Deep Reinforcement Learning\n",
    "\n",
    "> These successes motivate our approach to reinforcement learning. Our\n",
    "goal is to connect a reinforcement learning algorithm to a deep neural network which operates directly on RGB images and efficiently process training data by using stochastic gradient updates.\n",
    "> \n",
    "\n",
    "They **experience replay** where they store agent’s experiences from past time steps into a dataset which gets pooled, then Q-learning updates get applied to samples of experiences drawn from the dataset. The policy then greedily selects actions from the updated policy with exploration rate $\\epsilon$.\n",
    "\n",
    "![Screenshot 2024-12-05 at 3.10.14 PM.png](../../images/Screenshot_2024-12-05_at_3.10.14_PM.png)\n",
    "\n",
    "> Second, learning directly from consecutive samples is inefficient, due to the strong correlations between the samples; randomizing the samples breaks these correlations and therefore reduces the variance of the updates.\n",
    "> \n",
    "\n",
    "They randomize which samples they pull from at each training step to minimize the correlation between the results of different samples and extract maximal signal.\n",
    "\n",
    "**1. Preprocessing and Architecture**\n",
    "\n",
    "> Working directly with raw Atari frames, which are 210 × 160 pixel images with a 128 color palette, can be computationally demanding, so we apply a basic preprocessing step aimed at reducing the input dimensionality.\n",
    "> \n",
    "\n",
    "They reduce the dimensionality of the image first.\n",
    "\n",
    "> We instead use an architecture in which there is a separate output unit for each possible action, and only the state representation is an input to the neural network.\n",
    "> \n",
    "\n",
    "They compute Q-values for all actions in a given state at once which allows a single forward pass through the network for one time step.\n",
    "\n",
    "> We refer to convolutional networks trained with our approach as Deep Q-Networks (DQN).\n",
    "> \n",
    "\n",
    "They introduce the Deep Q-Network (DQN).\n",
    "\n",
    "### Experiments\n",
    "\n",
    "> So far, we have performed experiments on seven popular ATARI games – Beam Rider, Breakout, Enduro, Pong, Q*bert, Seaquest, Space Invaders.\n",
    "> \n",
    "\n",
    "They use the same network architecture for everything.\n",
    "\n",
    "They use a frame-skipping algorithm where the model observes and selects actions every k steps to make it more efficient.\n",
    "\n",
    "**1. Training and Stability**\n",
    "\n",
    "> In supervised learning, one can easily track the performance of a model during training by evaluating it on the training and validation sets. In reinforcement learning, however, accurately evaluating the progress of an agent during training can be challenging.\n",
    "> \n",
    "\n",
    "Measuring reward progress over time is far more noisy than supervised learning loss because changes in policy can result in completely different visited states.\n",
    "\n",
    "> Another, more stable, metric is the policy’s estimated action-value function Q, which provides an estimate of how much discounted reward the agent can obtain by following its policy from any given state.\n",
    "> \n",
    "\n",
    "![Screenshot 2024-12-05 at 3.16.58 PM.png](../../../images/Screenshot_2024-12-05_at_3.16.58_PM.png)\n",
    "\n",
    "> Our approach (labeled DQN) outperforms the other learning methods by a substantial margin on all seven games despite incorporating almost no prior knowledge about the inputs.\n",
    "> \n",
    "\n",
    "> Finally, we show that our method achieves better performance than an expert human player on Breakout, Enduro and Pong and it achieves close to human performance on Beam Rider.\n",
    "> \n",
    "\n",
    "![Screenshot 2024-12-05 at 3.17.44 PM.png](../../../images/Screenshot_2024-12-05_at_3.17.44_PM.png)\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "> We also presented a variant of online Q-learning that combines stochastic mini-batch updates with experience replay memory to ease the training of deep networks for RL.\n",
    "> \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
