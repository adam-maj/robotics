{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "> Designing the perception and control software for autonomous operation remains a major challenge, even for basic tasks.\n",
    "\n",
    "> In this article, we aim to answer the following question: can we acquire more effective policies for sensorimotor control if the perception system is trained jointly with the control policy, rather than separately?\n",
    "\n",
    "Separate policy and perception training requires the model to perform policy search with hand engineered features which are often prone to errors.\n",
    "\n",
    "> Successful applications of deep neural networks typically rely on large amounts of data and direct supervision of the output, neither of which is available in robotic control.\n",
    "\n",
    "> From the control perspective, a further complication is that observations from the robot’s sensors do not provide us with the full state of the system.\n",
    "\n",
    "> We address these challenges by developing a guided policy search algorithm for sensorimotor deep learning, as well as a novel CNN architecture designed for robotic control.\n",
    "\n",
    "Their CNN architecture has a spatial feature point transformation that improves spatial reasoning.\n",
    "\n",
    "> This allows us to train our policies with relatively modest amounts of data and only tens of minutes of real-world interaction time.\n",
    "\n",
    "### Related Work\n",
    "\n",
    "> Applications of deep learning in robotic control have been less prevalent in recent years than in visual recognition.\n",
    "\n",
    "> Pioneering early work on neural network control used small, simple networks, and has largely been supplanted by methods that use carefully designed policies that can be learned efficiently with reinforcement learning.\n",
    "\n",
    "Early control work using neural networks was replaced with hand designed robotic control policies.\n",
    "\n",
    "> CNNs have also been trained to play video games\n",
    ">\n",
    "> However, such methods have only been demonstrated on synthetic domains that lack the visual complexity of the real world, and require an impractical number of samples for real-world robotic learning.\n",
    "\n",
    "> Our method is sample efficient, requiring only minutes of interaction\n",
    "> time. To the best of our knowledge, this is the first method that can train deep visuomotor policies for complex, high-dimensional manipulation skills with direct torque control.\n",
    "\n",
    "Their method can be trained on only a few minutes of data, compared with reinforcement learning that requires huge amounts of samples in simulation.\n",
    "\n",
    "> Learning visuomotor policies on a real robot requires handling complex observations and high dimensional policy representations. We tackle these challenges using guided policy search.\n",
    "\n",
    "> In guided policy search, the policy is optimized using supervised learning, which scales gracefully with the dimensionality of the policy.\n",
    "\n",
    "### Background\n",
    "\n",
    "> The core component of our approach is a guided policy search algorithm that separates the problem of learning visuomotor policies into separate supervised learning and trajectory learning phases, each of which is easier than optimizing the policy directly.\n",
    "\n",
    "**1. Problem Formulation**\n",
    "\n",
    "The goal of policy search is to learn a policy $\\pi_\\theta(u_t|o_t)$ that allows the robot to perform actions $u_t$ based on observations $o_t$. The policy implicitly learns about the dynamics of the environment through observations. The goal is to minimize the task loss $\\ell(x_t, u_t)$.\n",
    "\n",
    "**2. Approach Summary**\n",
    "\n",
    "The system has two components. The first is a supervised learning algorithm that trains policies $\\pi_\\theta(u_t| o_t) = \\mathcal{N}(\\mu^\\pi(o_t), \\Sigma^\\pi(o_t))$ with $\\mu^\\pi(o_t)$ as a deep CNN and $\\Sigma^\\pi(o_t)$ as an observation independent learned covariance.\n",
    "\n",
    "The second is a trajectory centric RL algorithm that generates guiding distributions $p_i(u_t|x_t)$ for supervision.\n",
    "\n",
    "Supervised learning for policies doesn’t produce good long-horizon policies because the policy doesn’t know how to act out of distribution. The training data has to come from the policy’s state distribution to address this.\n",
    "\n",
    "> We achieve this by alternating between trajectory-centric RL and supervised learning.\n",
    "\n",
    "The RL stage provides supervision at states visited by the policy.\n",
    "\n",
    "![Screenshot 2024-11-08 at 10.16.47 AM.png](../../images/Screenshot_2024-11-08_at_10.16.47_AM.png)\n",
    "\n",
    "They use pre-training for the CNN to reduce the amount of necessary data.\n",
    "\n",
    "> The intuition behind our pre-training is that, although we ultimately seek to obtain sensorimotor policies that combine both vision and control, low-level aspects of vision can be initialized independently.\n",
    "\n",
    "They initialize the vision part of their network by training it to predict real elements of the image $x_t$ given the observations $o_t$ to bootstrap it with necessary skills.\n",
    "\n",
    "### Guided Policy Search with BADMM\n",
    "\n",
    "> Guided policy search transforms policy search into a supervised learning problem, where the training set is generated by a simple trajectory-centric RL algorithm.\n",
    "\n",
    "The end goal is to train the network that can operate on $\\pi_\\theta(u_t|o_t)$. To bootstrap the learning, they first collect ground truth data about the system and train the network with initialization $\\pi_\\theta(u_t|x_t)$ on more fine-grained control policies.\n",
    "\n",
    "Then, they use this pre-training to train the network on $\\pi_\\theta(u_t|x_t)$ that makes it more accurate. The downside is they have to collect real world data about the system.\n",
    "\n",
    "### End-to-End Visuomotor Policies\n",
    "\n",
    "> Guided policy search allows us to optimize complex, high-dimensional policies with raw observations, such as when the input to the policy consists of images from a robot’s onboard camera.\n",
    "\n",
    "> The guided policy search trajectory optimization phase uses the full state of the system, though the final policy only uses the observations.\n",
    "\n",
    "They train with full data on the true state. The network then learns to correlate observations with true state probably.\n",
    "\n",
    "> To speed up learning, we initialize both the vision layers in the policy and the trajectory distributions for guided policy search by leveraging the fully observed training setup.\n",
    "\n",
    "### Experimental Evaluation\n",
    "\n",
    "> We simulated 2D and 3D peg insertion, octopus arm control, and\n",
    "> planar swimming and walking.\n",
    "\n",
    "![Screenshot 2024-11-08 at 10.46.52 AM.png](../../images/Screenshot_2024-11-08_at_10.46.52_AM.png)\n",
    "\n",
    "![Screenshot 2024-11-08 at 10.47.14 AM.png](../../images/Screenshot_2024-11-08_at_10.47.14_AM.png)\n",
    "\n",
    "> These comparisons show that training even medium-sized neural network policies for continuous control tasks with a limited number of samples is very difficult for many prior policy search algorithms.\n",
    "\n",
    "> The visual processing layers of our architecture automatically learn features points using the spatial softmax and expectation operators. These feature points encapsulate all of the visual information received by the motor layers of the policy.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "> In this paper, we presented a method for learning robotic control policies that use raw input from a monocular camera.\n",
    "\n",
    "> These policies are represented by a novel convolutional neural network architecture, and can be trained end-to-end using our guided policy search algorithm, which decomposes the policy search problem in a trajectory optimization phase that uses full state information and a supervised learning phase that only uses the observations.\n",
    "\n",
    "> Our experimental results show that our method can execute complex manipulation skills, and that end-to-end training produces significant improvements in policy performance compared to using fixed vision layers trained for pose prediction.\n",
    "\n",
    "> The success of CNNs on exceedingly challenging vision tasks suggests that this class of models is capable of learning invariance to irrelevant distractor features.\n",
    "\n",
    "> Our method takes advantage of a known, fully observed state space during training. This is both a weakness and a strength.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
