{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notes\n",
        "\n",
        "> Existing systems for fine manipulation use expensive robots\n",
        "> and high-end sensors for precise state estimation. In this work, we seek to develop a low-cost system for fine manipulation that is, in contrast, accessible and reproducible.\n",
        "\n",
        "Dexterous manipulation systems are too expensive. ALOHA & ACT are meant to create a simpler and more cost effective dexterous manipulation system.\n",
        "\n",
        "> However, low-cost hardware is inevitably less precise than high-end platforms, making the sensing and planning challenge more pronounced. One promising direction to resolve this is to incorporate learning into the system.\n",
        "\n",
        "Need to mitigate the fact that cheaper hardware has less precision in sensing.\n",
        "\n",
        "> Humans also do not have industrial-grade proprioception, and yet we are able to perform delicate tasks by learning from closed-loop visual feedback and actively compensating for errors.\n",
        "\n",
        "Humans don’t use LiDAR or other expensive sensing setups, which means that the manipulation problem is solvable without them.\n",
        "\n",
        "They use a cheap teleoperation setup with 2 low-cost robotic arms and 3D printed components, leading to a teleoperation system that costs <$20k.\n",
        "\n",
        "> Small errors in the predicted action can incur large differences in the state, exacerbating the “compounding error” problem of imitation learning.\n",
        "\n",
        "If imitation learning actions are taken one at a time, errors in one earlier part of the action sequence compound into larger errors throughout the sequence.\n",
        "\n",
        "> To tackle this, we take inspiration from action chunking, a concept in psychology that describes how sequences of actions are grouped together as a chunk, and executed as one unit.\n",
        "\n",
        "They make a policy that predicts the action sequence for the next $k$ time steps instead of just 1 timestep, which reduces compounding errors.\n",
        "\n",
        "They use a transformer trained as a conditional VAE (CVAE) to implement action chunking. This is called an **action chunking transformer** (ACT).\n",
        "\n",
        "> The key contribution of this paper is a low-cost system for learning fine manipulation, comprising a teleoperation system and a novel imitation learning algorithm\n",
        "\n",
        "> The synergy between these two parts allows learning of 6 fine manipulation skills directly in the real-world, such as opening a translucent condiment cup and slotting a battery with 80-90% success, from only 10 minutes or 50 demonstration trajectories.\n",
        "\n",
        "ACT allows very fast and efficient fine manipulation training.\n",
        "\n",
        "### Related Work\n",
        "\n",
        "Imitation-learning allows robots to learn from experts. It’s commonly done with **behavior cloning**, where imitation learning is treated as a supervised learning problem.\n",
        "\n",
        "Behavior cloning suffers from compounding errors where errors from previous time steps build up, especially in fine manipulation.\n",
        "\n",
        "Previous solutions like DAgger (annotation is expensive), noise injection (reduces execution quality), and synthetic correction data (uses low dimensional visual data) all have issues.\n",
        "\n",
        "> We propose to reduce the effective horizon of tasks through action chunking, i.e., predicting an action sequence instead of a single action, and then ensemble across overlapping action chunks to produce trajectories that are both accurate and smooth.\n",
        "\n",
        "Previous bi-manual manipulation efforts originally used classical control using environment dynamics, then used learning like reinforcement learning and imitation learning.\n",
        "\n",
        "The ACT teleoperation setup uses joint-space mapping between the leader and follower robots, and has a setup of 3D printed parts that can be assembled in 2 hours.\n",
        "\n",
        "### ALOHA\n",
        "\n",
        "![Screenshot 2024-11-01 at 3.45.20 PM.png](../../images/Screenshot_2024-11-01_at_3.45.20_PM.png)\n",
        "\n",
        "> A Low-cost Open-source Hardware System for Bimanual Teleoperation\n",
        "\n",
        "The system should be [1] low-cost, [2] versatile, [3] user-friendly, [4] repairable, [5] easy-to-build.\n",
        "\n",
        "They use two ViperX 6-DoF robot arms which each cost $5600 as the robot arms, and replace the fingers to be better for fine manipulation.\n",
        "\n",
        "For the controllers, they noticed that joint-space manipulation is better than task-space manipulation (better to use a physical control system then VR controllers), so they use another set of WidowX arms as the “leader” arms, which each cost $3300.\n",
        "\n",
        "They also use 4 Logitech C922 webcams with 480 x 640 RGB image resolution streaming.\n",
        "\n",
        "ALOHA is good at precise tasks (like threading zip ties), contact-rich tasks (like inserting RAM into a motherboard/turning book pages), and dynamic tasks (like juggling a ping pong ball with a paddle).\n",
        "\n",
        "> Skills such as threading a zip tie, inserting RAM, and juggling ping pong ball, to our knowledge, are not available for existing teleoperation systems with 5-10x the budget.\n",
        "\n",
        "ALOHA is more effective than way more expensive systems.\n",
        "\n",
        "### Action Chunking with Transformers\n",
        "\n",
        "> Existing imitation learning algorithms perform poorly on fine-grained tasks that require high-frequency control and closed-loop feedback.\n",
        "\n",
        "To train ACT, they use human demonstrations using ALOHA.\n",
        "\n",
        "They use the joint positions of the _leader_ as the actions.\n",
        "\n",
        "A PID controller is used to cause the follower arm movement based on the leader movement.\n",
        "\n",
        "They use the joint positions of the _follower_ and the image feed from the 4 cameras as the observations.\n",
        "\n",
        "Then, they train ACT to predict the sequence of future actions given the observations.\n",
        "\n",
        "**1. Action Chunking and Temporal Ensemble**\n",
        "\n",
        "> We are inspired by action chunking, a neuroscience concept where individual actions are grouped together and executed as one unit, making them more efficient to store and execute.\n",
        "\n",
        "The model policy predicts the next $k$ time steps of actions, effectively predicting a chunk of $k$ actions, which results in a $k$-fold reduction in the effective horizon of the task.\n",
        "\n",
        "The policy models $\\pi_\\theta(a_{t:t+k}|s_t)$ instead of $\\pi_\\theta(a_t|s_t)$.\n",
        "\n",
        "A single-step model would also struggle with temporal confusion like pauses, whereas pauses in an individual action chunk wouldn’t be an issue.\n",
        "\n",
        "> To improve smoothness and a void discrete switching between, executing and observing, we query the policy at every time step.\n",
        "\n",
        "Instead of running inference for $k$ actions every $k$ time steps which would be clunky, they run inference every time-step and take a **temporal ensemble** of all the action predictions at that time step with a weight average $w_i = \\textrm{exp}(-m * i)$.\n",
        "\n",
        "They are aggregating action predictions all for the same time step.\n",
        "\n",
        "**2. Modeling Human Data**\n",
        "\n",
        "> Another challenge that arises is learning from noisy human demonstrations. Given the same observation, a human can use different trajectories to solve the task. Humans will also be more stochastic in regions where precision matters less.\n",
        "\n",
        "Human demonstrations can have high variance in the how demonstrators execute tasks. The model has to learn to be precise when it matters but learn a general distribution of approaches when more freedom is permissible. This is the perfect structure for VAE representations (where the model can learn the distributions to model the signal and noise in different action sequences).\n",
        "\n",
        "> Thus, it is important for the policy to focus on regions where high precision matters.\n",
        "\n",
        "The policy uses a conditional variational autoencoder (CVAE) to generate an action sequence based on observations.\n",
        "\n",
        "The encoder is only used for training and predicts the mean and variance of the internal variable $z$’s distribution based on the current action sequence and observations (uses just proprioceptive data instead of images for simplicity).\n",
        "\n",
        "The decoder uses both $z$ and current observations to predict the action sequence.\n",
        "\n",
        "**3. Implementing ACT**\n",
        "\n",
        "> We implement the CVAE encoder and decoder with transformers, as transformers are designed for both synthesizing information across a sequence and generating new sequences.\n",
        "\n",
        "The CVAE encoder takes in the $k$ next target actions from the demonstration dataset with the [CLS] token and generates the $z$ style variable.\n",
        "\n",
        "The decoder then takes the $z$ style variable and the current observations and predicts the next $k$ actions.\n",
        "\n",
        "The CVAE decoder uses ResNet image encoders and a transformer encoder to synthesize information from different camera view points, joint positions, and the style variable and a transformer decoder to generate a coherent action sequence.\n",
        "\n",
        "The transformer output dimensions is $k \\times 512$ which is then projected down into $k \\times 14$ where each value corresponds with the predicted joint position for each action time step.\n",
        "\n",
        "![Screenshot 2024-11-01 at 6.21.54 PM.png](../../images/Screenshot_2024-11-01_at_6.21.54_PM.png)\n",
        "\n",
        "### Experiments\n",
        "\n",
        "They use 6 real-world tasks and 2 fine manipulation tasks in MuJoCo which they use for simulation.\n",
        "\n",
        "![Screenshot 2024-11-01 at 4.26.41 PM.png](../../images/Screenshot_2024-11-01_at_4.26.41_PM.png)\n",
        "\n",
        "They collect 50 episodes for each of the tasks, where each episode is 8-14s which corresponds to 400-700 time steps given the 50Hz control frequency.\n",
        "\n",
        "All the different demonstrations are stochastic.\n",
        "\n",
        "ACT performs far better than other state of the art models on all of these tasks.\n",
        "\n",
        "![Screenshot 2024-11-01 at 4.29.28 PM.png](../../images/Screenshot_2024-11-01_at_4.29.28_PM.png)\n",
        "\n",
        "Compared with BeT and RT-1 which discretize the action space [output is a categorical distribution over discrete bins], ACT directly predicts continuous actions which is necessary for fine manipulation.\n",
        "\n",
        "The model also performs better with action chunking and temporal ensembling.\n",
        "\n",
        "![Screenshot 2024-11-01 at 4.45.01 PM.png](../../images/Screenshot_2024-11-01_at_4.45.01_PM.png)\n",
        "\n",
        "They also find that higher control frequency leads to faster task completion.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "> We present a low-cost system for fine manipulation, comprising a teleoperation system ALOHA and a novel imitation learning algorithm ACT.\n",
        "\n",
        "> The synergy between these two parts allows us to learn fine manipulation skills directly in the real world, such as opening a translucent condiment cup and slotting a battery with a 80-90% success rate and around 10 min of demonstrations."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
